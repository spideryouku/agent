# HelloAgents æœ¬åœ°éƒ¨ç½²æŒ‡å—

## ğŸ  æœ¬åœ°éƒ¨ç½²æ¦‚è¿°

HelloAgentsç°åœ¨å…¨é¢æ”¯æŒæœ¬åœ°LLMéƒ¨ç½²æ–¹æ¡ˆï¼ŒåŒ…æ‹¬Ollamaã€vLLMå’Œå…¶ä»–OpenAIå…¼å®¹çš„æœ¬åœ°æœåŠ¡ã€‚

## ğŸš€ æ”¯æŒçš„æœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆ

### 1. Ollama éƒ¨ç½²

#### å®‰è£…Ollama
```bash
# macOS/Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# ä» https://ollama.ai/download ä¸‹è½½å®‰è£…åŒ…
```

#### å¯åŠ¨æ¨¡å‹
```bash
# ä¸‹è½½å¹¶è¿è¡ŒLlama 3.2
ollama run llama3.2

# æˆ–å…¶ä»–æ¨¡å‹
ollama run qwen2.5:7b
ollama run codellama:7b
```

#### HelloAgentsé…ç½®
```env
# æ–¹å¼1ï¼šä½¿ç”¨ä¸“ç”¨ç¯å¢ƒå˜é‡
OLLAMA_API_KEY=ollama
OLLAMA_HOST=http://localhost:11434/v1

# æ–¹å¼2ï¼šä½¿ç”¨ç»Ÿä¸€é…ç½®
LLM_MODEL_ID=llama3.2
LLM_API_KEY=ollama
LLM_BASE_URL=http://localhost:11434/v1
```

```python
from hello_agents import HelloAgentsLLM, SimpleAgent

# è‡ªåŠ¨æ£€æµ‹ä¸ºollama
llm = HelloAgentsLLM()
agent = SimpleAgent("LlamaåŠ©æ‰‹", llm)
response = agent.run("ä½ å¥½ï¼")
```

### 2. vLLM éƒ¨ç½²

#### å®‰è£…vLLM
```bash
pip install vllm
```

#### å¯åŠ¨vLLMæœåŠ¡
```bash
# å¯åŠ¨Llama-2-7Bæ¨¡å‹
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --port 8000

# æˆ–å¯åŠ¨Qwenæ¨¡å‹
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --port 8000
```

#### HelloAgentsé…ç½®
```env
# æ–¹å¼1ï¼šä½¿ç”¨ä¸“ç”¨ç¯å¢ƒå˜é‡
VLLM_API_KEY=vllm
VLLM_HOST=http://localhost:8000/v1

# æ–¹å¼2ï¼šä½¿ç”¨ç»Ÿä¸€é…ç½®
LLM_MODEL_ID=meta-llama/Llama-2-7b-chat-hf
LLM_API_KEY=vllm
LLM_BASE_URL=http://localhost:8000/v1
```

```python
from hello_agents import HelloAgentsLLM, SimpleAgent

# è‡ªåŠ¨æ£€æµ‹ä¸ºvllm
llm = HelloAgentsLLM()
agent = SimpleAgent("vLLMåŠ©æ‰‹", llm)
response = agent.run("ä½ å¥½ï¼")
```

### 3. FastChat éƒ¨ç½²

#### å®‰è£…FastChat
```bash
pip install fschat
```

#### å¯åŠ¨FastChatæœåŠ¡
```bash
# å¯åŠ¨æ§åˆ¶å™¨
python -m fastchat.serve.controller

# å¯åŠ¨æ¨¡å‹worker
python -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5

# å¯åŠ¨OpenAIå…¼å®¹APIæœåŠ¡å™¨
python -m fastchat.serve.openai_api_server --host localhost --port 8000
```

#### HelloAgentsé…ç½®
```env
LLM_MODEL_ID=vicuna-7b-v1.5
LLM_API_KEY=local
LLM_BASE_URL=http://localhost:8000/v1
```

### 4. Text Generation WebUI

#### å®‰è£…Text Generation WebUI
```bash
git clone https://github.com/oobabooga/text-generation-webui.git
cd text-generation-webui
pip install -r requirements.txt
```

#### å¯åŠ¨æœåŠ¡
```bash
python server.py --api --listen --port 7860
```

#### HelloAgentsé…ç½®
```env
LLM_MODEL_ID=your-model-name
LLM_API_KEY=local
LLM_BASE_URL=http://localhost:7860/v1
```

### 5. å…¶ä»–æœ¬åœ°éƒ¨ç½²

å¯¹äºä»»ä½•æä¾›OpenAIå…¼å®¹APIçš„æœ¬åœ°æœåŠ¡ï¼š

```env
LLM_MODEL_ID=your-custom-model
LLM_API_KEY=local
LLM_BASE_URL=http://localhost:PORT/v1
```

## ğŸ” è‡ªåŠ¨æ£€æµ‹é€»è¾‘

HelloAgentsä¼šæ ¹æ®ä»¥ä¸‹è§„åˆ™è‡ªåŠ¨æ£€æµ‹æœ¬åœ°éƒ¨ç½²ï¼š

### 1. ç‰¹å®šæœåŠ¡æ£€æµ‹
- `OLLAMA_API_KEY` æˆ– `OLLAMA_HOST` â†’ ollama
- `VLLM_API_KEY` æˆ– `VLLM_HOST` â†’ vllm

### 2. URLæ¨¡å¼æ£€æµ‹
- `localhost:11434` æˆ–åŒ…å« `ollama` â†’ ollama
- `localhost:8000` ä¸”åŒ…å« `vllm` â†’ vllm
- `localhost:8080`ã€`localhost:7860` â†’ local
- å…¶ä»–localhostç«¯å£ â†’ local

### 3. APIå¯†é’¥æ£€æµ‹
- `LLM_API_KEY=ollama` â†’ ollama
- `LLM_API_KEY=vllm` â†’ vllm
- `LLM_API_KEY=local` â†’ local

## ğŸ“‹ é…ç½®ç¤ºä¾‹

### å®Œæ•´çš„æœ¬åœ°éƒ¨ç½²é…ç½®æ–‡ä»¶
```env
# ============================================================================
# HelloAgents æœ¬åœ°éƒ¨ç½²é…ç½®
# ============================================================================

# Ollamaé…ç½®
# LLM_MODEL_ID=llama3.2
# LLM_API_KEY=ollama
# LLM_BASE_URL=http://localhost:11434/v1

# vLLMé…ç½®
# LLM_MODEL_ID=meta-llama/Llama-2-7b-chat-hf
# LLM_API_KEY=vllm
# LLM_BASE_URL=http://localhost:8000/v1

# FastChaté…ç½®
# LLM_MODEL_ID=vicuna-7b-v1.5
# LLM_API_KEY=local
# LLM_BASE_URL=http://localhost:8000/v1

# Text Generation WebUIé…ç½®
# LLM_MODEL_ID=your-model
# LLM_API_KEY=local
# LLM_BASE_URL=http://localhost:7860/v1

# é€šç”¨æœ¬åœ°éƒ¨ç½²é…ç½®
LLM_MODEL_ID=your-local-model
LLM_API_KEY=local
LLM_BASE_URL=http://localhost:8080/v1
LLM_TIMEOUT=120  # æœ¬åœ°éƒ¨ç½²å¯èƒ½éœ€è¦æ›´é•¿è¶…æ—¶æ—¶é—´
```

### Pythonä½¿ç”¨ç¤ºä¾‹
```python
from hello_agents import HelloAgentsLLM, SimpleAgent, ReActAgent, ToolRegistry

# 1. åŸºç¡€å¯¹è¯
llm = HelloAgentsLLM()  # è‡ªåŠ¨æ£€æµ‹æœ¬åœ°éƒ¨ç½²
agent = SimpleAgent("æœ¬åœ°åŠ©æ‰‹", llm)
response = agent.run("ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")

# 2. å·¥å…·è°ƒç”¨ï¼ˆæœ¬åœ°æ¨¡å‹ä¹Ÿæ”¯æŒï¼‰
from hello_agents.tools.builtin import calculate

tool_registry = ToolRegistry()
tool_registry.register_function("calculate", "æ•°å­¦è®¡ç®—", calculate)

react_agent = ReActAgent("æœ¬åœ°å·¥å…·åŠ©æ‰‹", llm, tool_registry)
result = react_agent.run("è®¡ç®— 123 * 456 çš„ç»“æœ")

# 3. æ£€æŸ¥é…ç½®
print(f"Provider: {llm.provider}")
print(f"Model: {llm.model}")
print(f"Base URL: {llm.base_url}")
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **è¿æ¥å¤±è´¥**
   ```python
   # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨
   import requests
   response = requests.get("http://localhost:11434/v1/models")
   print(response.status_code)
   ```

2. **æ¨¡å‹æœªæ‰¾åˆ°**
   ```bash
   # Ollama: æ£€æŸ¥å¯ç”¨æ¨¡å‹
   ollama list
   
   # vLLM: ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®
   ls ~/.cache/huggingface/transformers/
   ```

3. **è¶…æ—¶é—®é¢˜**
   ```env
   # å¢åŠ è¶…æ—¶æ—¶é—´
   LLM_TIMEOUT=300
   ```

### æ€§èƒ½ä¼˜åŒ–

1. **GPUåŠ é€Ÿ**
   ```bash
   # Ollama GPUæ”¯æŒ
   ollama run llama3.2  # è‡ªåŠ¨ä½¿ç”¨GPU
   
   # vLLM GPUæ”¯æŒ
   python -m vllm.entrypoints.openai.api_server \
       --model meta-llama/Llama-2-7b-chat-hf \
       --tensor-parallel-size 2
   ```

2. **å†…å­˜ä¼˜åŒ–**
   ```bash
   # ä½¿ç”¨é‡åŒ–æ¨¡å‹
   ollama run llama3.2:7b-instruct-q4_0
   ```

## ğŸ¯ æœ€ä½³å®è·µ

1. **é€‰æ‹©åˆé€‚çš„éƒ¨ç½²æ–¹æ¡ˆ**
   - **Ollama**: ç®€å•æ˜“ç”¨ï¼Œé€‚åˆå¿«é€Ÿä½“éªŒ
   - **vLLM**: é«˜æ€§èƒ½ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒ
   - **FastChat**: åŠŸèƒ½ä¸°å¯Œï¼Œæ”¯æŒå¤šç§æ¨¡å‹
   - **Text Generation WebUI**: å›¾å½¢ç•Œé¢ï¼Œé€‚åˆç ”ç©¶

2. **æ¨¡å‹é€‰æ‹©å»ºè®®**
   - **7Bæ¨¡å‹**: é€‚åˆ16GBå†…å­˜
   - **13Bæ¨¡å‹**: é€‚åˆ32GBå†…å­˜
   - **70Bæ¨¡å‹**: éœ€è¦å¤šGPUæˆ–é‡åŒ–

3. **é…ç½®å»ºè®®**
   - æœ¬åœ°éƒ¨ç½²å»ºè®®å¢åŠ è¶…æ—¶æ—¶é—´
   - ä½¿ç”¨åˆé€‚çš„æ¨¡å‹å¤§å°
   - è€ƒè™‘ä½¿ç”¨é‡åŒ–æ¨¡å‹èŠ‚çœå†…å­˜

ç°åœ¨HelloAgentså®Œå…¨æ”¯æŒæœ¬åœ°LLMéƒ¨ç½²ï¼Œè®©æ‚¨å¯ä»¥åœ¨æœ¬åœ°ç¯å¢ƒä¸­äº«å—AIåŠ©æ‰‹çš„å¼ºå¤§åŠŸèƒ½ï¼ğŸš€
