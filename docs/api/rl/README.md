# HelloAgents RLè®­ç»ƒæŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨HelloAgentsçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒåŠŸèƒ½ã€‚

## ğŸ“š è¯¦ç»†æ–‡æ¡£

- **[æ•°æ®é›†API](datasets.md)** - æ•°æ®é›†åŠ è½½å’Œå¤„ç†
- **[å¥–åŠ±å‡½æ•°API](rewards.md)** - å¥–åŠ±å‡½æ•°åˆ›å»ºå’Œä½¿ç”¨
- **[è®­ç»ƒå™¨API](trainers.md)** - SFTå’ŒGRPOè®­ç»ƒå™¨
- **[RLTrainingTool](rl_training_tool.md)** - ç»Ÿä¸€è®­ç»ƒå·¥å…·(æ¨è)

## ç›®å½•

- [å®‰è£…](#å®‰è£…)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è®­ç»ƒç®—æ³•](#è®­ç»ƒç®—æ³•)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [é«˜çº§é…ç½®](#é«˜çº§é…ç½®)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

## å®‰è£…

### æ–¹å¼1ï¼šå®‰è£…å®Œæ•´çš„RLåŠŸèƒ½ï¼ˆæ¨èï¼‰

```bash
pip install hello-agents[rl]
```

è¿™å°†å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š
- `trl`: Transformer Reinforcement Learningåº“
- `transformers`: HuggingFace Transformers
- `torch`: PyTorch
- `datasets`: HuggingFace Datasets
- `accelerate`: åˆ†å¸ƒå¼è®­ç»ƒåŠ é€Ÿ
- `peft`: LoRAç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒ
- `bitsandbytes`: é‡åŒ–æ”¯æŒ
- `wandb`: è®­ç»ƒç›‘æ§ï¼ˆå¯é€‰ï¼‰
- `tensorboard`: TensorBoardæ”¯æŒï¼ˆå¯é€‰ï¼‰

### æ–¹å¼2ï¼šå•ç‹¬å®‰è£…TRL

```bash
pip install trl
```

### éªŒè¯å®‰è£…

```python
from hello_agents.rl import TRL_AVAILABLE

if TRL_AVAILABLE:
    print("âœ… TRLå·²å®‰è£…ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ")
else:
    print("âŒ TRLæœªå®‰è£…")
```

## å¿«é€Ÿå¼€å§‹

### ä½¿ç”¨å·¥å…·æ¥å£

```python
from hello_agents.tools import RLTrainingTool

# åˆ›å»ºRLè®­ç»ƒå·¥å…·
rl_tool = RLTrainingTool()

# SFTè®­ç»ƒ
result = rl_tool.run({
    "algorithm": "sft",
    "model_name": "Qwen/Qwen2-0.5B-Instruct",
    "dataset": "gsm8k",
    "max_samples": 100,
    "num_epochs": 3,
    "output_dir": "./output/sft"
})

print(result)
```

### åŠ è½½æ•°æ®é›†

```python
# åŠ è½½SFTæ ¼å¼æ•°æ®é›†
result = rl_tool.run({
    "action": "load_dataset",
    "format": "sft",
    "split": "train",
    "max_samples": 100
})

# åŠ è½½RLæ ¼å¼æ•°æ®é›†
result = rl_tool.run({
    "action": "load_dataset",
    "format": "rl",
    "split": "train",
    "max_samples": 100,
    "model_name": "Qwen/Qwen3-0.6B"
})
```

### åœ¨Agentä¸­ä½¿ç”¨

```python
from hello_agents.agents import SimpleAgent
from hello_agents.tools import RLTrainingTool
from hello_agents.core import LLMConfig

# åˆ›å»ºAgent
agent = SimpleAgent(
    name="TrainingAgent",
    llm_config=LLMConfig(model="gpt-4o-mini"),
    tools=[RLTrainingTool()]
)

# è®©Agentæ‰§è¡Œè®­ç»ƒä»»åŠ¡
response = agent.run(
    "è¯·ç”¨SFTç®—æ³•è®­ç»ƒä¸€ä¸ªQwen2-0.5Bæ¨¡å‹ï¼Œä½¿ç”¨gsm8kæ•°æ®é›†ï¼Œè®­ç»ƒ3è½®"
)
```

## è®­ç»ƒç®—æ³•

### SFT (Supervised Fine-Tuning)

**ç›‘ç£å¾®è°ƒ**ï¼Œè®©æ¨¡å‹å­¦ä¼šéµå¾ªæŒ‡ä»¤å’ŒåŸºæœ¬çš„æ¨ç†æ ¼å¼ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š
- æ¨¡å‹åˆå§‹å¯¹é½
- å­¦ä¹ ç‰¹å®šä»»åŠ¡æ ¼å¼
- ä½œä¸ºRLè®­ç»ƒçš„åŸºç¡€

**ç¤ºä¾‹**ï¼š
```python
rl_tool.run({
    "action": "train",
    "algorithm": "sft",
    "model_name": "Qwen/Qwen3-0.6B",
    "max_samples": 1000,
    "num_epochs": 3,
    "output_dir": "./output/sft",
    "use_lora": True,
    "batch_size": 4
})
```

### GRPO (Group Relative Policy Optimization)

**ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–**ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**ä¼˜åŠ¿**ï¼š
- ä¸éœ€è¦Value Modelï¼Œæ›´ç®€å•
- å†…å­˜å ç”¨æ›´å°‘
- è®­ç»ƒé€Ÿåº¦æ›´å¿«
- æ€§èƒ½æ¥è¿‘PPO

**é€‚ç”¨åœºæ™¯**ï¼š
- ä¼˜åŒ–æ¨ç†èƒ½åŠ›
- æé«˜ç­”æ¡ˆå‡†ç¡®ç‡
- Agentic RLè®­ç»ƒ

**ç¤ºä¾‹**ï¼š
```python
rl_tool.run({
    "action": "train",
    "algorithm": "grpo",
    "model_name": "Qwen/Qwen3-0.6B",
    "max_samples": 500,
    "num_epochs": 3,
    "output_dir": "./output/grpo",
    "use_lora": True,
    "batch_size": 2
})
```

### PPO (Proximal Policy Optimization)

**è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–**ï¼Œç»å…¸çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚

**çŠ¶æ€**ï¼šğŸš§ å¼€å‘ä¸­

**è¯´æ˜**ï¼šPPOéœ€è¦é¢å¤–çš„Value Modelï¼Œå®ç°æ›´å¤æ‚ã€‚å»ºè®®ä½¿ç”¨GRPOä½œä¸ºæ›¿ä»£ã€‚

## ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šå®Œæ•´è®­ç»ƒæµç¨‹

æ¨èçš„è®­ç»ƒæµç¨‹ï¼šå…ˆSFTï¼Œå†GRPO

```python
from hello_agents.tools import RLTrainingTool

rl_tool = RLTrainingTool()

# æ­¥éª¤1ï¼šSFTè®­ç»ƒ
print("æ­¥éª¤1ï¼šSFTè®­ç»ƒ...")
sft_result = rl_tool.run({
    "action": "train",
    "algorithm": "sft",
    "model_name": "Qwen/Qwen3-0.6B",
    "max_samples": 1000,
    "num_epochs": 3,
    "output_dir": "./output/sft"
})

# æ­¥éª¤2ï¼šGRPOè®­ç»ƒï¼ˆä½¿ç”¨SFTåçš„æ¨¡å‹ï¼‰
print("æ­¥éª¤2ï¼šGRPOè®­ç»ƒ...")
grpo_result = rl_tool.run({
    "action": "train",
    "algorithm": "grpo",
    "model_name": "./output/sft",  # ä½¿ç”¨SFTè®­ç»ƒåçš„æ¨¡å‹
    "max_samples": 500,
    "num_epochs": 3,
    "output_dir": "./output/grpo"
})

print("è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹: ./output/grpo")
```

### ç¤ºä¾‹2ï¼šå¿«é€Ÿæµ‹è¯•

ä½¿ç”¨å°‘é‡æ ·æœ¬å¿«é€Ÿæµ‹è¯•è®­ç»ƒæµç¨‹ï¼š

```python
# å¿«é€ŸSFTæµ‹è¯•ï¼ˆ10ä¸ªæ ·æœ¬ï¼Œ1è½®ï¼‰
rl_tool.run({
    "action": "train",
    "algorithm": "sft",
    "model_name": "Qwen/Qwen3-0.6B",
    "max_samples": 10,
    "num_epochs": 1,
    "output_dir": "./output/test_sft"
})
```

### ç¤ºä¾‹3ï¼šä½¿ç”¨LoRAå‡å°‘æ˜¾å­˜

```python
# ä½¿ç”¨LoRAè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ
rl_tool.run({
    "action": "train",
    "algorithm": "sft",
    "model_name": "Qwen/Qwen3-0.6B",
    "use_lora": True,  # å¯ç”¨LoRA
    "batch_size": 2,   # å°æ‰¹æ¬¡
    "output_dir": "./output/sft_lora"
})
```

## é«˜çº§é…ç½®

### ä½¿ç”¨åº•å±‚API

å¦‚æœéœ€è¦æ›´å¤šæ§åˆ¶ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨åº•å±‚APIï¼š

```python
from hello_agents.rl import (
    SFTTrainerWrapper,
    GRPOTrainerWrapper,
    TrainingConfig,
    create_sft_dataset,
    create_rl_dataset,
    create_accuracy_reward
)

# åˆ›å»ºé…ç½®
config = TrainingConfig(
    model_name="Qwen/Qwen2-0.5B-Instruct",
    output_dir="./output/custom",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=5e-5,
    use_lora=True,
    lora_r=16,
    lora_alpha=32
)

# SFTè®­ç»ƒ
dataset = create_sft_dataset(max_samples=1000)
trainer = SFTTrainerWrapper(config=config, dataset=dataset)
trainer.train()
trainer.save_model()

# GRPOè®­ç»ƒ
rl_dataset = create_rl_dataset(max_samples=500)
reward_fn = create_accuracy_reward()
grpo_trainer = GRPOTrainerWrapper(
    config=config,
    dataset=rl_dataset,
    reward_fn=reward_fn
)
grpo_trainer.train()
grpo_trainer.save_model()
```

### è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

```python
from hello_agents.rl import MathRewardFunction

# åˆ›å»ºè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
class CustomReward(MathRewardFunction):
    def __call__(self, completions: List[str], **kwargs) -> List[float]:
        ground_truths = kwargs.get("ground_truth", [])
        rewards = []
        for completion, truth in zip(completions, ground_truths):
            # è‡ªå®šä¹‰å¥–åŠ±é€»è¾‘
            pred = self.extract_answer(completion)
            if pred and self.compare_answers(pred, truth):
                reward = 1.0
            else:
                reward = 0.0
            rewards.append(reward)
        return rewards

# ä½¿ç”¨è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
reward_fn = CustomReward()
```

## å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒéœ€è¦å¤šå°‘æ˜¾å­˜ï¼Ÿ

**A**: å–å†³äºæ¨¡å‹å¤§å°å’Œé…ç½®ï¼š

- **Qwen3-0.6B + LoRA**: çº¦4-6GBï¼ˆå•GPUå¯è®­ç»ƒï¼‰
- **Qwen3-0.6B å…¨å‚æ•°**: çº¦8-12GB
- **Qwen2-1.5B + LoRA**: çº¦8-12GB
- **Qwen2-7B + LoRA**: çº¦16-24GB

**å»ºè®®**ï¼š
- ä½¿ç”¨LoRAå‡å°‘æ˜¾å­˜å ç”¨
- å‡å°batch_size
- å¯ç”¨gradient_checkpointing

### Q2: è®­ç»ƒéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ

**A**: å–å†³äºæ•°æ®é‡å’Œç¡¬ä»¶ï¼š

- **100æ ·æœ¬ï¼Œ1è½®ï¼Œå•GPU**: çº¦5-10åˆ†é’Ÿ
- **1000æ ·æœ¬ï¼Œ3è½®ï¼Œå•GPU**: çº¦30-60åˆ†é’Ÿ
- **å…¨é‡GSM8Kï¼ˆ7.5Kï¼‰ï¼Œ3è½®ï¼Œå•GPU**: çº¦3-6å°æ—¶

### Q3: SFTå’ŒGRPOæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A**:
- **SFT**: ç›‘ç£å­¦ä¹ ï¼Œç›´æ¥å­¦ä¹ æ­£ç¡®ç­”æ¡ˆçš„æ ¼å¼
- **GRPO**: å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ¨ç†è¿‡ç¨‹

**æ¨èæµç¨‹**: å…ˆSFTå­¦ä¹ æ ¼å¼ï¼Œå†GRPOä¼˜åŒ–èƒ½åŠ›

### Q4: ä¸ºä»€ä¹ˆæ¨èGRPOè€Œä¸æ˜¯PPOï¼Ÿ

**A**: GRPOçš„ä¼˜åŠ¿ï¼š
- ä¸éœ€è¦Value Modelï¼Œå®ç°æ›´ç®€å•
- å†…å­˜å ç”¨æ›´å°‘
- è®­ç»ƒé€Ÿåº¦æ›´å¿«
- æ€§èƒ½æ¥è¿‘PPOï¼ˆ90%+ï¼‰

### Q5: å¦‚ä½•è¯„ä¼°è®­ç»ƒæ•ˆæœï¼Ÿ

**A**: å¯ä»¥ä½¿ç”¨è¯„ä¼°å·¥å…·ï¼š

```python
from hello_agents.rl import evaluate_rewards, create_accuracy_reward

# è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°
test_dataset = create_rl_dataset(split="test", max_samples=100)
reward_fn = create_accuracy_reward()

# ç”Ÿæˆé¢„æµ‹å¹¶è¯„ä¼°
# ... (éœ€è¦åŠ è½½è®­ç»ƒåçš„æ¨¡å‹å¹¶ç”Ÿæˆé¢„æµ‹)
```

### Q6: è®­ç»ƒå¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼š

1. **æ˜¾å­˜ä¸è¶³**:
   - å¯ç”¨LoRA: `use_lora=True`
   - å‡å°batch_size
   - ä½¿ç”¨gradient_checkpointing

2. **TRLæœªå®‰è£…**:
   ```bash
   pip install hello-agents[rl]
   ```

3. **æ•°æ®é›†ä¸‹è½½å¤±è´¥**:
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - ä½¿ç”¨é•œåƒæº
   - æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†

## å‚è€ƒèµ„æº

- [TRLå®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/trl)
- [GRPOè®ºæ–‡](https://arxiv.org/abs/2402.03300)
- [GSM8Kæ•°æ®é›†](https://huggingface.co/datasets/openai/gsm8k)
- [Qwenæ¨¡å‹](https://huggingface.co/Qwen)

## ä¸‹ä¸€æ­¥

- æŸ¥çœ‹å®Œæ•´ç¤ºä¾‹: `examples/rl_training_example.py`
- äº†è§£Agentic RLç†è®º: `docs/chapter11/`
- æ¢ç´¢æ›´å¤šè®­ç»ƒç®—æ³•: DPO, KTO, ORPOç­‰

