#!/usr/bin/env python3
"""
ç¬¬åä¸€ç« ï¼šAgentic RLå®Œæ•´æ•™å­¦ç¤ºä¾‹

æœ¬æ–‡ä»¶æ•´åˆäº†ç¬¬åä¸€ç« ä¸­ä»‹ç»çš„Agentic RLè®­ç»ƒæµç¨‹çš„æ‰€æœ‰å®ç”¨æ¡ˆä¾‹:

ğŸ¯ æ ¸å¿ƒå†…å®¹
- SFT (Supervised Fine-Tuning) è®­ç»ƒ
- GRPO (Group Relative Policy Optimization) è®­ç»ƒ
- æ•°æ®é›†åŠ è½½å’Œå¤„ç†
- å¥–åŠ±å‡½æ•°è®¾è®¡å’Œä½¿ç”¨
- LoRAå‚æ•°é«˜æ•ˆå¾®è°ƒ
- æ¨¡å‹è¯„ä¼°å’Œåˆ†æ
- å®Œæ•´è®­ç»ƒæµç¨‹å®æˆ˜

ğŸ“š å­¦ä¹ ç›®æ ‡:
âœ… ç†è§£LLMè®­ç»ƒçš„å®Œæ•´æµç¨‹(é¢„è®­ç»ƒâ†’SFTâ†’RL)
âœ… æŒæ¡SFTå’ŒGRPOçš„è®­ç»ƒæ–¹æ³•
âœ… å­¦ä¼šè®¾è®¡å’Œä½¿ç”¨å¥–åŠ±å‡½æ•°
âœ… äº†è§£LoRAçš„é…ç½®å’Œä¼˜åŒ–
âœ… æŒæ¡æ¨¡å‹è¯„ä¼°å’Œé”™è¯¯åˆ†ææ–¹æ³•
âœ… èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„Agentic RLè®­ç»ƒæµç¨‹

ğŸš€ è¿è¡Œæ–¹å¼:
python examples/chapter11_RL.py

ğŸ“¦ ä¾èµ–å®‰è£…:
pip install hello-agents[rl]
# æˆ–è€…
pip install transformers datasets trl peft accelerate

ğŸ‘¨â€ğŸ’» ä½œè€…: HelloAgents æ•™å­¦å›¢é˜Ÿ
ğŸ“… æ›´æ–°: 2025å¹´1æœˆ
"""

import sys
import os
import json
from pathlib import Path
from typing import Dict, Any, List, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from hello_agents.tools import RLTrainingTool


# ============================================================================
# ç¤ºä¾‹1: å¿«é€Ÿå…¥é—¨ - æœ€ç®€å•çš„è®­ç»ƒæµç¨‹
# ============================================================================

def example_01_quick_start():
    """
    ç¤ºä¾‹1: å¿«é€Ÿå…¥é—¨
    
    æœ€ç®€å•çš„SFTâ†’GRPOâ†’è¯„ä¼°æµç¨‹
    é€‚åˆåˆå­¦è€…å¿«é€Ÿä½“éªŒå®Œæ•´è®­ç»ƒæµç¨‹
    
    é…ç½®:
    - æ¨¡å‹: Qwen/Qwen3-0.6B (å°æ¨¡å‹,å¿«é€Ÿè®­ç»ƒ)
    - æ ·æœ¬æ•°: 10ä¸ª (å¿«é€Ÿæµ‹è¯•)
    - è®­ç»ƒè½®æ•°: 1è½®
    - é¢„è®¡æ—¶é—´: 2-3åˆ†é’Ÿ
    """
    print("="*80)
    print("ç¤ºä¾‹1: å¿«é€Ÿå…¥é—¨ - æœ€ç®€å•çš„è®­ç»ƒæµç¨‹")
    print("="*80)
    
    tool = RLTrainingTool()
    
    # æ­¥éª¤1: SFTè®­ç»ƒ
    print("\næ­¥éª¤1: SFTè®­ç»ƒ(å­¦ä¹ åŸºç¡€æ¨ç†æ ¼å¼)")
    print("-"*80)
    
    sft_result = tool.run({
        "action": "train",
        "algorithm": "sft",
        "model_name": "Qwen/Qwen3-0.6B",
        "output_dir": "./output/quick_start/sft",
        "max_samples": 10,
        "num_epochs": 1,
        "batch_size": 2,
        "use_lora": True,
    })
    
    print("âœ… SFTè®­ç»ƒå®Œæˆ!")
    print(json.dumps(json.loads(sft_result), indent=2, ensure_ascii=False))
    
    # æ­¥éª¤2: GRPOè®­ç»ƒ
    print("\næ­¥éª¤2: GRPOè®­ç»ƒ(å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–)")
    print("-"*80)
    
    grpo_result = tool.run({
        "action": "train",
        "algorithm": "grpo",
        "model_name": "./output/quick_start/sft",
        "output_dir": "./output/quick_start/grpo",
        "max_samples": 5,
        "num_epochs": 1,
        "batch_size": 1,
        "use_lora": True,
    })
    
    print("âœ… GRPOè®­ç»ƒå®Œæˆ!")
    print(json.dumps(json.loads(grpo_result), indent=2, ensure_ascii=False))
    
    # æ­¥éª¤3: è¯„ä¼°æ¨¡å‹
    print("\næ­¥éª¤3: è¯„ä¼°è®­ç»ƒåçš„æ¨¡å‹")
    print("-"*80)
    
    eval_result = tool.run({
        "action": "evaluate",
        "model_path": "./output/quick_start/grpo",
        "max_samples": 10,
        "use_lora": True,
    })
    
    eval_data = json.loads(eval_result)
    print("âœ… è¯„ä¼°å®Œæˆ!")
    print(f"  å‡†ç¡®ç‡: {eval_data['accuracy']:.2%}")
    print(f"  å¹³å‡å¥–åŠ±: {eval_data['average_reward']:.3f}")
    print(f"  æµ‹è¯•æ ·æœ¬æ•°: {eval_data['num_samples']}")
    
    print("\n" + "="*80)
    print("ğŸ‰ å¿«é€Ÿå…¥é—¨å®Œæˆ!")
    print("="*80)


# ============================================================================
# ç¤ºä¾‹2: æ•°æ®é›†åŠ è½½å’Œæ¢ç´¢
# ============================================================================

def example_02_dataset_loading():
    """
    ç¤ºä¾‹2: æ•°æ®é›†åŠ è½½å’Œæ¢ç´¢
    
    æ¼”ç¤ºå¦‚ä½•åŠ è½½å’ŒæŸ¥çœ‹GSM8Kæ•°æ®é›†
    äº†è§£SFTå’ŒRLä¸¤ç§æ•°æ®æ ¼å¼çš„åŒºåˆ«
    """
    print("="*80)
    print("ç¤ºä¾‹2: æ•°æ®é›†åŠ è½½å’Œæ¢ç´¢")
    print("="*80)
    
    tool = RLTrainingTool()
    
    # åŠ è½½SFTæ ¼å¼æ•°æ®é›†
    print("\n1. åŠ è½½SFTæ ¼å¼æ•°æ®é›†")
    print("-"*80)
    
    sft_data = tool.run({
        "action": "load_dataset",
        "format_type": "sft",
        "split": "train",
        "max_samples": 3,
    })
    
    print("SFTæ•°æ®æ ¼å¼:")
    print(json.dumps(json.loads(sft_data), indent=2, ensure_ascii=False))
    
    # åŠ è½½RLæ ¼å¼æ•°æ®é›†
    print("\n2. åŠ è½½RLæ ¼å¼æ•°æ®é›†")
    print("-"*80)
    
    rl_data = tool.run({
        "action": "load_dataset",
        "format_type": "rl",
        "split": "train",
        "max_samples": 3,
    })
    
    print("RLæ•°æ®æ ¼å¼:")
    print(json.dumps(json.loads(rl_data), indent=2, ensure_ascii=False))
    
    print("\n" + "="*80)
    print("æ•°æ®é›†åŠ è½½å®Œæˆ!")
    print("="*80)


# ============================================================================
# ç¤ºä¾‹3: å¥–åŠ±å‡½æ•°è®¾è®¡
# ============================================================================

def example_03_reward_functions():
    """
    ç¤ºä¾‹3: å¥–åŠ±å‡½æ•°è®¾è®¡
    
    æ¼”ç¤ºä¸åŒç±»å‹çš„å¥–åŠ±å‡½æ•°:
    - å‡†ç¡®ç‡å¥–åŠ± (accuracy)
    - é•¿åº¦æƒ©ç½šå¥–åŠ± (length_penalty)
    - æ­¥éª¤å¥–åŠ± (step)
    """
    print("="*80)
    print("ç¤ºä¾‹3: å¥–åŠ±å‡½æ•°è®¾è®¡")
    print("="*80)
    
    tool = RLTrainingTool()
    
    # 1. å‡†ç¡®ç‡å¥–åŠ±
    print("\n1. å‡†ç¡®ç‡å¥–åŠ±")
    print("-"*80)
    
    accuracy_reward = tool.run({
        "action": "create_reward",
        "reward_type": "accuracy",
    })
    
    print("å‡†ç¡®ç‡å¥–åŠ±å‡½æ•°:")
    print(json.dumps(json.loads(accuracy_reward), indent=2, ensure_ascii=False))
    
    # 2. é•¿åº¦æƒ©ç½šå¥–åŠ±
    print("\n2. é•¿åº¦æƒ©ç½šå¥–åŠ±")
    print("-"*80)
    
    length_reward = tool.run({
        "action": "create_reward",
        "reward_type": "length_penalty",
        "penalty_weight": 0.01,
    })
    
    print("é•¿åº¦æƒ©ç½šå¥–åŠ±å‡½æ•°:")
    print(json.dumps(json.loads(length_reward), indent=2, ensure_ascii=False))
    
    # 3. æ­¥éª¤å¥–åŠ±
    print("\n3. æ­¥éª¤å¥–åŠ±")
    print("-"*80)
    
    step_reward = tool.run({
        "action": "create_reward",
        "reward_type": "step",
        "step_bonus": 0.1,
    })
    
    print("æ­¥éª¤å¥–åŠ±å‡½æ•°:")
    print(json.dumps(json.loads(step_reward), indent=2, ensure_ascii=False))
    
    print("\n" + "="*80)
    print("å¥–åŠ±å‡½æ•°åˆ›å»ºå®Œæˆ!")
    print("="*80)


# ============================================================================
# ç¤ºä¾‹4: LoRAé…ç½®ä¼˜åŒ–
# ============================================================================

def example_04_lora_configuration():
    """
    ç¤ºä¾‹4: LoRAé…ç½®ä¼˜åŒ–
    
    æ¼”ç¤ºä¸åŒLoRAé…ç½®çš„æ•ˆæœ:
    - å¿«é€Ÿå®éªŒé…ç½® (r=8)
    - æ ‡å‡†é…ç½® (r=16)
    - é«˜æ€§èƒ½é…ç½® (r=32)
    """
    print("="*80)
    print("ç¤ºä¾‹4: LoRAé…ç½®ä¼˜åŒ–")
    print("="*80)
    
    tool = RLTrainingTool()
    
    configs = {
        "å¿«é€Ÿå®éªŒ": {"lora_r": 8, "lora_alpha": 16, "batch_size": 8},
        "æ ‡å‡†é…ç½®": {"lora_r": 16, "lora_alpha": 32, "batch_size": 4},
        "é«˜æ€§èƒ½": {"lora_r": 32, "lora_alpha": 64, "batch_size": 2},
    }
    
    print("\nLoRAé…ç½®å¯¹æ¯”:")
    print("-"*80)
    for name, config in configs.items():
        print(f"\n{name}:")
        print(f"  lora_r: {config['lora_r']}")
        print(f"  lora_alpha: {config['lora_alpha']}")
        print(f"  batch_size: {config['batch_size']}")
    
    # ä½¿ç”¨æ ‡å‡†é…ç½®è¿›è¡Œè®­ç»ƒ
    print("\nä½¿ç”¨æ ‡å‡†é…ç½®è¿›è¡Œè®­ç»ƒ:")
    print("-"*80)
    
    result = tool.run({
        "action": "train",
        "algorithm": "sft",
        "model_name": "Qwen/Qwen3-0.6B",
        "output_dir": "./output/lora_standard",
        "max_samples": 10,
        "num_epochs": 1,
        "use_lora": True,
        "lora_r": 16,
        "lora_alpha": 32,
        "batch_size": 4,
    })
    
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print(json.dumps(json.loads(result), indent=2, ensure_ascii=False))
    
    print("\n" + "="*80)
    print("LoRAé…ç½®ä¼˜åŒ–å®Œæˆ!")
    print("="*80)


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("\n" + "ğŸ“ "*20)
    print("ç¬¬åä¸€ç« : Agentic RL å®Œæ•´æ•™å­¦ç¤ºä¾‹")
    print("ğŸ“ "*20 + "\n")
    
    examples = [
        ("ç¤ºä¾‹1: å¿«é€Ÿå…¥é—¨", example_01_quick_start),
        ("ç¤ºä¾‹2: æ•°æ®é›†åŠ è½½", example_02_dataset_loading),
        ("ç¤ºä¾‹3: å¥–åŠ±å‡½æ•°è®¾è®¡", example_03_reward_functions),
        ("ç¤ºä¾‹4: LoRAé…ç½®ä¼˜åŒ–", example_04_lora_configuration),
    ]
    
    print("å¯ç”¨ç¤ºä¾‹:")
    for i, (name, _) in enumerate(examples, 1):
        print(f"  {i}. {name}")
    
    print("\né€‰æ‹©è¦è¿è¡Œçš„ç¤ºä¾‹ (è¾“å…¥æ•°å­—,æˆ–æŒ‰Enterè¿è¡Œæ‰€æœ‰ç¤ºä¾‹):")
    choice = input("> ").strip()
    
    if choice == "":
        # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
        for name, func in examples:
            print(f"\n\n{'='*80}")
            print(f"è¿è¡Œ: {name}")
            print('='*80)
            func()
    elif choice.isdigit() and 1 <= int(choice) <= len(examples):
        # è¿è¡Œé€‰å®šçš„ç¤ºä¾‹
        name, func = examples[int(choice) - 1]
        print(f"\n\n{'='*80}")
        print(f"è¿è¡Œ: {name}")
        print('='*80)
        func()
    else:
        print("âŒ æ— æ•ˆçš„é€‰æ‹©!")
        return
    
    print("\n\n" + "ğŸ‰ "*20)
    print("æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
    print("ğŸ‰ "*20 + "\n")


if __name__ == "__main__":
    main()

